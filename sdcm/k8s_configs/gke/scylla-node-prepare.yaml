# ClusterRole for cpu-policy-daemonset.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pv-setup
rules:
  - apiGroups:
      - scylla.scylladb.com
    resources:
      - scyllaclusters
    verbs:
      - get
      - list
      - watch
---
# ServiceAccount for cpu-policy daemonset.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pv-setup
  namespace: default
---
# Bind cpu-policy daemonset ServiceAccount with ClusterRole.
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pv-setup
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: pv-setup
subjects:
- kind: ServiceAccount
  name: pv-setup
  namespace: default
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-setup
spec:
  selector:
    matchLabels:
      app: node-setup
  template:
    metadata:
      labels:
        app: node-setup
    spec:
      hostPID: true
      serviceAccountName: pv-setup
      tolerations:
      - key: role
        operator: Equal
        value: scylla-clusters
        effect: NoSchedule
      containers:
      # NOTE: the 'node-setup' container's script was taken from the following place:
      #       https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/local-ssd#run-local-volume-static-provisioner
      #       https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/be7c6372/examples/gke-daemonset-raid-disks.yaml
      - name: node-setup
        image: gcr.io/google-containers/startup-script:v1
        securityContext:
          privileged: true
        env:
          - name: STARTUP_SCRIPT
            value: |
              set -o errexit
              set -o nounset
              set -o pipefail

              devices=()
              for ssd in /dev/disk/by-id/google-local-ssd-block*; do
                if [ -e "${ssd}" ]; then
                  devices+=("${ssd}")
                fi
              done
              if [ "${#devices[@]}" -eq 0 ]; then
                echo "No Local NVMe SSD disks found."
                exit 0
              fi

              seen_arrays=(/dev/md/*)
              device=${seen_arrays[0]}
              echo "Setting RAID array with Local SSDs on device ${device}"
              if [ ! -e "$device" ]; then
                device="/dev/md/0"
                echo "y" | mdadm --create "${device}" --level=0 --force --raid-devices=${#devices[@]} "${devices[@]}"
              fi

              if ! tune2fs -l "${device}" ; then
                echo "Formatting '${device}'"
                mkfs.xfs "${device}"
              fi

              mountpoint=/mnt/raid-disks/disk0
              mkdir -p "${mountpoint}"
              echo "Mounting '${device}' at '${mountpoint}'"
              mount -o discard,defaults "${device}" "${mountpoint}"
              chmod a+w "${mountpoint}"
        volumeMounts:
          - name: hostfs
            mountPath: /mnt/hostfs
            mountPropagation: Bidirectional
      - name: pv-setup
        image: bitnami/kubectl:1.21.4
        imagePullPolicy: Always
        env:
          - name: HOSTFS
            value: /mnt/hostfs
          - name: KUBELET_CONFIG_PATH
            value: /home/kubernetes/kubelet-config.yaml
        securityContext:
          privileged: true
          runAsUser: 0
        volumeMounts:
          - name: hostfs
            mountPath: /mnt/hostfs
            mountPropagation: Bidirectional
        command:
          - "/bin/bash"
          - "-c"
          - "--"
        args:
          - |
            set -ex
            if [ ! -f "$HOSTFS$KUBELET_CONFIG_PATH" ]; then
                echo "Kublet config not found"
                exit 1
            fi

            TOKEN_PATH=$(find /run/secrets/kubernetes.io -name token | grep token -m1)
            TOKEN=$(cat $TOKEN_PATH)
            CA_CRT=$TOKEN_PATH/../ca.crt
            kubectl config set-cluster scylla --server=https://kubernetes.default --certificate-authority=$CA_CRT
            kubectl config set-credentials qa@scylladb.com --token=$TOKEN
            kubectl config set-context scylla --cluster=scylla --user=qa@scylladb.com
            kubectl config use-context scylla

            # Create directories for each Scylla cluster on each K8S node which will host PVs
            while true; do
              if [[ -z $(mount | grep  "/dev/md0") ]]; then
                sleep 5;
                continue
              fi
              for i in $(seq -f "pv-%02g" $( kubectl get scyllaclusters -A --field-selector metadata.name!=scylla-manager --no-headers | grep -c " " ) ); do
                if [[ ! -d "/mnt/hostfs/mnt/raid-disks/disk0/${i}" ]]; then
                  mkdir "/mnt/hostfs/mnt/raid-disks/disk0/${i}"
                fi
                ( mount | grep "/mnt/hostfs/mnt/raid-disks/disk0/${i} " 2>/dev/null 1>&2 ) || \
                mount --bind "/mnt/hostfs/mnt/raid-disks/disk0/${i}"{,}
              done
              sleep 30
            done
      volumes:
      - name: hostfs
        hostPath:
          path: /
