# Test duration (min). Parameter used to keep instances produced by tests that are supposed to run longer than 24 hours from being killed
test_duration: 60

# cassandra-stress commands. You can specify everything but the -node parameter, which is going to be provided by the test suite infrastructure
# multiple commands can passed as a list
prepare_write_cmd: "cassandra-stress write no-warmup cl=QUORUM n=3000 -schema 'replication(factor=3)' -port jmx=6868 -mode cql3 native -rate threads=1 -pop seq=1..3000"
stress_read_cmd: ["cassandra-stress read no-warmup cl=QUORUM duration=10m -schema 'replication(factor=3)' -port jmx=6868 -mode cql3 native -rate threads=1 -pop 'dist=gauss(1..3000,1500,150)'"]
stress_cmd: ["cassandra-stress mixed cl=QUORUM duration=10m -schema 'replication(factor=3) compaction(strategy=SizeTieredCompactionStrategy)' -port jmx=6868 -mode cql3 native -rate threads=2 -pop seq=1..3000 -log interval=5"]

# run cql command 'select count(*) ks.cf from rand loader node to rand db_node in parallel thread
run_fullscan: 'keyspace1.standard1, 5' # 'ks.cf|random, interval(min)''

# Number list of database nodes in multiple date centers.
n_db_nodes: 1

# Number list of loader nodes in multiple date centers
n_loaders: 1
# If you want to use more than 1 loader node, I recommend increasing the size of the DB instance (n_db_nodes parameter),
# since 't2.micro' nodes tend to struggle with a lot of load.

# Number list of monitor nodes in multiple date centers.
n_monitor_nodes: 1

# Across how many keyspaces to spread the load
keyspace_num: 1

# Nemesis class to use (possible types in sdcm.nemesis). Example: StopStartMonkey
# NoOpMonkey - Doesn't activate any Nemesis (default)
# NotSpotNemesis
# StopWaitStartMonkey
# StopStartMonkey
# RestartThenRepairNodeMonkey
# MultipleHardRebootNodeMonkey
# HardRebootNodeMonkey
# SoftRebootNodeMonkey
# DrainerMonkey
# CorruptThenRepairMonkey
# CorruptThenRebuildMonkey
# DecommissionMonkey
# NoCorruptRepairMonkey
# MajorCompactionMonkey
# RefreshMonkey
# RefreshBigMonkey
# EnospcMonkey
# EnospcAllNodesMonkey
# NodeToolCleanupMonkey

# ChaosMonkey
# LimitedChaosMonkey
# AllMonkey
# MdcChaosMonkey

# UpgradeNemesis
# UpgradeNemesisOneNode
# RollbackNemesis
# TODO: finish the docs for each one
nemesis_class_name: 'NoOpMonkey'

# Nemesis sleep interval to use if None provided specifically in the test
nemesis_interval: 15

# TODO:
nemesis_during_prepare: 'true'

# Prefix for your AWS/GCE VMs (handy for telling instances from different
# users apart). If you leave this empty, the prefix will be your unix username.
user_prefix: ''

# Failure/post test behavior
# Default: Destroy AWS instances and credentials (destroy)
# Keep AWS instances running and leave credentials alone (keep)
# Stop AWS instances and leave credentials alone (stop)
failure_post_behavior: destroy

# Space node threshold before starting nemesis (bytes)
# The default value is 6GB (6x1024^3 bytes)
# This value is supposed to reproduce
# https://github.com/scylladb/scylla/issues/1140
space_node_threshold: 6442450944

# Type of IP used to connect to machine instances.
# This depends on whether you are running your tests from a machine inside
# your cloud provider, where it makes sense to use 'private', or outside (use 'public')
# Default: Use public IPs to connect to instances (public)
# Use private IPs to connect to instances (private)
ip_ssh_connections: 'public'

# The new db binary will be uploaded to db instance to replace the one
# provided by the ami. This is useful to test out a new scylla version
# without making a new ami.
# update_db_binary: $path_of_the_new_scylla_binary
update_db_binary: ''

# The new db packages will be uploaded to db instance to update the one
# provided by the ami. This is useful to test out a new scylla version
# without making a new ami via rpm package updates.
# update_db_packages: $path_of_folder_with_packages
update_db_packages: ''

es_url:
es_user:
es_password:

# UPGRADE TESTS
# Repo file that is used in upgrade tests
new_scylla_repo: 'http://downloads.scylladb.com/rpm/centos/scylla-3.0.repo'

# New scylla version to update that should exist in `new_scylla_repo`
# For example: 3.0.rc2, 3.0.0, 3.0.1, etc...
# By default upgrade till latest major version
new_version: ''
# The new scylla packages will be uploaded to db instance to update the one
# provided by the ami. This is used when we want to upgrade to a version
# that is not yet in the repository.
# upgrade_node_packages: $path_of_folder_with_packages
upgrade_node_packages: ''

# capturing nodetool API via tcpdump
tcpdump: false

backends: !mux
    libvirt: !mux
        cluster_backend: 'libvirt'
        libvirt_uri: 'qemu:///system'
        libvirt_bridge: 'virbr0'
        scylla_repo: 'http://downloads.scylladb.com/rpm/centos/scylla-1.3.repo'
        libvirt_loader_image: '/path/to/your/loader_base_image.qcow2'
        libvirt_loader_image_user: 'centos'
        libvirt_loader_image_password: '123456'
        libvirt_loader_os_type: 'linux'
        libvirt_loader_os_variant: 'centos7.0'
        libvirt_loader_memory: 2048
        libvirt_db_image: '/path/to/your/db_base_image.qcow2'
        libvirt_db_image_user: 'centos'
        libvirt_db_image_password: '123456'
        libvirt_db_os_type: 'linux'
        libvirt_db_os_variant: 'centos7.0'
        libvirt_db_memory: 2048
        libvirt_monitor_image: '/path/to/your/monitor_base_image.qcow2'
        libvirt_monitor_image_user: 'centos'
        libvirt_monitor_image_password: '123456'
        libvirt_monitor_os_type: 'linux'
        libvirt_monitor_os_variant: 'centos7.0'
        libvirt_monitor_memory: 2048
    aws: !mux
        # What is the backend that the suite will use to get machines from.
        cluster_backend: 'aws'
        # From 0.19 on, iotune will require bigger disk, so let's use a big
        # loader instance by default.
        instance_type_loader: 'c3.large'
        # Size of AWS monitor instance
        instance_type_monitor: t2.small
        ami_id_db_scylla_desc: '1.3-dev'
        us_west_1:
            user_credentials_path: ''
            region_name: 'us-west-1'
            security_group_ids: 'sg-dcd785b9'
            subnet_id: 'subnet-10a04c75'
            ami_id_db_scylla: 'ami-19eca679'
            ami_db_scylla_user: 'centos'
            ami_id_loader: 'ami-0a2ac860155a18864' # Loader dedicated AMI
            ami_loader_user: 'centos'
            ami_id_db_cassandra: 'ami-3cf7c979'
            ami_db_cassandra_user: 'ubuntu'
            ami_id_monitor: 'ami-074e2d6769f445be5' # Official CentOS Linux 7 x86_64 HVM EBS ENA 1901_01
            ami_monitor_user: 'centos'
        us_west_2:
            user_credentials_path: ''
            region_name: 'us-west-2'
            security_group_ids: 'sg-81703ae4'
            subnet_id: 'subnet-5207ee37'
            ami_id_db_scylla: 'ami-ec3e9e8c'
            ami_db_scylla_user: 'centos'
            ami_id_loader: 'ami-0e4373a35e7d9ccdb' # Loader dedicated AMI
            ami_loader_user: 'centos'
            ami_id_db_cassandra: 'ami-1cff962c'
            ami_db_cassandra_user: 'ubuntu'
            ami_id_monitor: 'ami-01ed306a12b7d1c96' # Official CentOS Linux 7 x86_64 HVM EBS ENA 1901_01
            ami_monitor_user: 'centos'
        us_east_1:
            user_credentials_path: ''
            region_name: 'us-east-1'
            security_group_ids: 'sg-c5e1f7a0'
            subnet_id: 'subnet-ec4a72c4'
            ami_id_db_scylla: 'ami-79d3f06e'
            ami_db_scylla_user: 'centos'
            ami_id_loader: 'ami-0803fc42f8277925f' # Loader dedicated AMI
            ami_loader_user: 'centos'
            ami_id_db_cassandra: 'ami-ada2b6c4'
            ami_db_cassandra_user: 'ubuntu'
            ami_id_monitor: 'ami-02eac2c0129f6376b' # Official CentOS Linux 7 x86_64 HVM EBS ENA 1901_01
            ami_monitor_user: 'centos'
        #multiple datacenters
        us_east_1_and_us_west_2:
            user_credentials_path: ''
            region_name: 'us-east-1 us-west-2'
            security_group_ids: 'sg-c5e1f7a0 sg-81703ae4'
            subnet_id: 'subnet-ec4a72c4 subnet-5207ee37'
            ami_id_db_scylla: 'ami-79d3f06e ami-ec3e9e8c'
            ami_db_scylla_user: 'centos'
            ami_id_loader: 'ami-0803fc42f8277925f' # Loader dedicated AMI
            ami_loader_user: 'centos'
            ami_id_db_cassandra: 'ami-ada2b6c4'
            ami_db_cassandra_user: 'ubuntu'
            ami_id_monitor: 'ami-02eac2c0129f6376b' # Official CentOS Linux 7 x86_64 HVM EBS ENA 1901_01
            ami_monitor_user: 'centos'

    openstack: !mux
        cluster_backend: 'openstack'
        openstack_auth_version: '3.x_password'
        openstack_auth_url: 'http://1.2.3.4:5000'
        openstack_user: 'put_your_user_here'
        openstack_password: 'put_your_pass_here'
        openstack_tenant: 'put_your_tenant_here'
        openstack_service_type: 'compute'
        openstack_service_name: 'nova'
        openstack_service_region: 'put_your_region_here'
        openstack_network: 'put_your_network_here'
        # Openstack image: CentOS7
        openstack_image: '56450641-b01e-4357-b7a9-8b02d6286d90'
        openstack_image_username: 'centos'
        openstack_user_credentials: '/path/to/user/credentials.pem'
        openstack_instance_type_db: 'm1.small'
        openstack_instance_type_loader: 'm1.small'
        openstack_instance_type_monitor: 'm1.small'
        scylla_repo: 'http://downloads.scylladb.com/rpm/centos/scylla-1.4.repo'

    gce: !mux
        cluster_backend: 'gce'
        # Put here the path to your user credentials (usually $HOME/.ssh/google_compute_engine). Generated using the gcloud utility
        user_credentials_path: ''
        gce_image: 'https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/family/centos-7'
        # Your e-mail user (if foo@yourcompany.com, foo is the user)
        gce_image_username: 'your-gce-username'
        #
        # gce_root_disk_size_xx units: GB
        # gce_root_disk_type: 'pd-standard' or 'pd-ssd' (far more expensive)
        #
        # DB instance and disk information
        # For large data set tests, 1500 GB of pd-standard yield 180 MB/s, which seems like a good deal
        gce_instance_type_db: 'n1-standard-2'
        gce_root_disk_type_db: 'pd-ssd'
        gce_root_disk_size_db: 50
        gce_n_local_ssd_disk_db: 1
        # Loader instance and disk information
        gce_instance_type_loader: 'n1-highcpu-4'
        gce_root_disk_type_loader: 'pd-standard'
        gce_root_disk_size_loader: 50
        gce_n_local_ssd_disk_loader: 0
        # Monitor instance and disk information
        gce_instance_type_monitor: 'n1-standard-1'
        gce_root_disk_type_monitor: 'pd-standard'
        gce_root_disk_size_monitor: 50
        gce_n_local_ssd_disk_monitor: 0
        scylla_repo: 'http://downloads.scylladb.com/rpm/centos/scylla-1.4.repo'
        us_east_1:
          gce_datacenter: 'us-east1-b'
        # multiple datacenters
        # us_east_1_and_asias_northeast_1:
        #   gce_datacenter: 'us-east1-b asia-northeast1-a'

    docker: !mux
        cluster_backend: 'docker'
        docker_image: 'scylladb/scylla'
        user_credentials_path: '~/.ssh/scylla-test'

databases: !mux
    cassandra:
        db_type: cassandra
        instance_type_db: 'm3.large'
        # Spawn a monitor node
        n_monitor_nodes: 1
    scylla:
        db_type: scylla
        # Let's use c3.large since we're using iotune
        # and we'll stress the DB nodes more thoroughly
        instance_type_db: 'c3.large'
        # Spawn a monitor node
        n_monitor_nodes: 1
